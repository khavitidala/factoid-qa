{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finetuning FacQA\n",
    "Question Answering (Modul Answer Finder) adalah Sebuah task NLP (Natural Language Processing) yang bertujuan untuk menghasilkan jawaban (factoid) berdasar dua input yang tersedia yaitu pertanyaan pengguna dan paragraf/kalimat yang merupakan sumber jawaban. Tipe factoid adalah jawaban yang berbentuk serangkaian kata yang merupakan bagian dari sebuah kalimat. Tipe factoid yang tersedia pada dataset ini adalah (`Person`, `Organization`, `Location`, `Datetime`, `Quantity`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install transformers>=2.9.0 pandas>=0.25.3 numpy>=1.17.4 scikit-learn>=0.22.1 nltk==3.4.5 unidecode>=1.1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "# sys.path.append('../')\n",
    "# os.chdir('../')\n",
    "\n",
    "import re\n",
    "import random\n",
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertConfig, BertTokenizer, AutoTokenizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from conlleval import conll_evaluation\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from modules.word_classification import BertForWordClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# common functions\n",
    "###\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "def count_param(module, trainable=False):\n",
    "    if trainable:\n",
    "        return sum(p.numel() for p in module.parameters() if p.requires_grad)\n",
    "    else:\n",
    "        return sum(p.numel() for p in module.parameters())\n",
    "    \n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def metrics_to_string(metric_dict):\n",
    "    string_list = []\n",
    "    for key, value in metric_dict.items():\n",
    "        string_list.append('{}:{:.2f}'.format(key, value))\n",
    "    return ' '.join(string_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Forward function for word classification\n",
    "def forward_word_classification(model, batch_data, i2w, is_test=False, device='cpu', **kwargs):\n",
    "    # Unpack batch data\n",
    "    if len(batch_data) == 4:\n",
    "        (subword_batch, mask_batch, subword_to_word_indices_batch, label_batch) = batch_data\n",
    "        token_type_batch = None\n",
    "    elif len(batch_data) == 5:\n",
    "        (subword_batch, mask_batch, token_type_batch, subword_to_word_indices_batch, label_batch) = batch_data\n",
    "    \n",
    "    # Prepare input & label\n",
    "    subword_batch = torch.LongTensor(subword_batch)\n",
    "    mask_batch = torch.FloatTensor(mask_batch)\n",
    "    token_type_batch = torch.LongTensor(token_type_batch) if token_type_batch is not None else None\n",
    "    subword_to_word_indices_batch = torch.LongTensor(subword_to_word_indices_batch)\n",
    "    label_batch = torch.LongTensor(label_batch)\n",
    "\n",
    "    if device == \"cuda\":\n",
    "        subword_batch = subword_batch.cuda()\n",
    "        mask_batch = mask_batch.cuda()\n",
    "        token_type_batch = token_type_batch.cuda() if token_type_batch is not None else None\n",
    "        subword_to_word_indices_batch = subword_to_word_indices_batch.cuda()\n",
    "        label_batch = label_batch.cuda()\n",
    "\n",
    "    # Forward model\n",
    "    outputs = model(subword_batch, subword_to_word_indices_batch, attention_mask=mask_batch, token_type_ids=token_type_batch, labels=label_batch)\n",
    "    loss, logits = outputs[:2]\n",
    "    \n",
    "    # generate prediction & label list\n",
    "    list_hyps = []\n",
    "    list_labels = []\n",
    "    hyps_list = torch.topk(logits, k=1, dim=-1)[1].squeeze(dim=-1)\n",
    "    for i in range(len(hyps_list)):\n",
    "        hyps, labels = hyps_list[i].tolist(), label_batch[i].tolist()        \n",
    "        list_hyp, list_label = [], []\n",
    "        for j in range(len(hyps)):\n",
    "            if labels[j] == -100:\n",
    "                break\n",
    "            else:\n",
    "                list_hyp.append(i2w[hyps[j]])\n",
    "                list_label.append(i2w[labels[j]])\n",
    "        list_hyps.append(list_hyp)\n",
    "        list_labels.append(list_label)\n",
    "        \n",
    "    return loss, list_hyps, list_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def qa_factoid_metrics_fn(list_hyp, list_label):\n",
    "    metrics = {}\n",
    "    acc, pre, rec, f1, tm_pre, tm_rec, tm_f1 = conll_evaluation(list_hyp, list_label)\n",
    "    metrics[\"ACC\"] = acc\n",
    "    metrics[\"F1\"] = tm_f1\n",
    "    metrics[\"REC\"] = tm_rec\n",
    "    metrics[\"PRE\"] = tm_pre\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####\n",
    "# QA Factoid ITB\n",
    "#####\n",
    "class QAFactoidDataset(Dataset):\n",
    "    # Static constant variable\n",
    "    LABEL2INDEX = {'O':0, 'B':1, 'I':2}\n",
    "    INDEX2LABEL = {0:'O', 1:'B', 2:'I'}\n",
    "    NUM_LABELS = 3\n",
    "    \n",
    "    def load_dataset(self, path):\n",
    "        # Read file\n",
    "        dataset = pd.read_csv(path)\n",
    "        \n",
    "        # Question and passage are a list of words and seq_label is list of B/I/O\n",
    "        dataset['question'] = dataset['question'].apply(lambda x: eval(x))\n",
    "        dataset['passage'] = dataset['passage'].apply(lambda x: eval(x))\n",
    "        dataset['seq_label'] = dataset['seq_label'].apply(lambda x: [self.LABEL2INDEX[l] for l in eval(x)])\n",
    "\n",
    "        return dataset\n",
    "    \n",
    "    def __init__(self, dataset_path, tokenizer, *args, **kwargs):\n",
    "        self.data = self.load_dataset(dataset_path)\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        data = self.data.loc[index,:]\n",
    "        question, passage, seq_label = data['question'],  data['passage'], data['seq_label']\n",
    "        \n",
    "        # Add CLS token\n",
    "        subwords = [self.tokenizer.cls_token_id]\n",
    "        subword_to_word_indices = [-1] # For CLS\n",
    "        token_type_ids = [0]\n",
    "        \n",
    "        # Add subwords for question\n",
    "        for word_idx, word in enumerate(question):\n",
    "            subword_list = self.tokenizer.encode(word, add_special_tokens=False)\n",
    "            subword_to_word_indices += [-1 for i in range(len(subword_list))]\n",
    "            token_type_ids += [0 for i in range(len(subword_list))]\n",
    "            subwords += subword_list\n",
    "            \n",
    "        # Add intermediate SEP token\n",
    "        subwords += [self.tokenizer.sep_token_id]\n",
    "        subword_to_word_indices += [-1]\n",
    "        token_type_ids += [0]\n",
    "        \n",
    "        # Add subwords\n",
    "        for word_idx, word in enumerate(passage):\n",
    "            subword_list = self.tokenizer.encode(word, add_special_tokens=False)\n",
    "            subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "            token_type_ids += [1 for i in range(len(subword_list))]\n",
    "            subwords += subword_list\n",
    "            \n",
    "        # Add last SEP token\n",
    "        subwords += [self.tokenizer.sep_token_id]\n",
    "        subword_to_word_indices += [-1]\n",
    "        token_type_ids += [1]\n",
    "        \n",
    "        return np.array(subwords), np.array(token_type_ids), np.array(subword_to_word_indices), np.array(seq_label), ' '.join(question) + \"|\" + ' '.join(passage)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "class QAFactoidDataLoader(DataLoader):\n",
    "    def __init__(self, max_seq_len=512, *args, **kwargs):\n",
    "        super(QAFactoidDataLoader, self).__init__(*args, **kwargs)\n",
    "        self.collate_fn = self._collate_fn\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "    def _collate_fn(self, batch):\n",
    "        batch_size = len(batch)\n",
    "        max_seq_len = max(map(lambda x: len(x[0]), batch))\n",
    "        max_seq_len = min(self.max_seq_len, max_seq_len)\n",
    "        max_tgt_len = max(map(lambda x: len(x[3]), batch))\n",
    "        \n",
    "        subword_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        mask_batch = np.zeros((batch_size, max_seq_len), dtype=np.float32)\n",
    "        token_type_batch = np.zeros((batch_size, max_seq_len), dtype=np.int64)\n",
    "        subword_to_word_indices_batch = np.full((batch_size, max_seq_len), -1, dtype=np.int64)\n",
    "        seq_label_batch = np.full((batch_size, max_tgt_len), -100, dtype=np.int64)\n",
    "\n",
    "        seq_list = []\n",
    "        for i, (subwords, token_type_ids, subword_to_word_indices, seq_label, raw_seq) in enumerate(batch):\n",
    "            subwords = subwords[:max_seq_len]\n",
    "            subword_to_word_indices = subword_to_word_indices[:max_seq_len]\n",
    "\n",
    "            subword_batch[i,:len(subwords)] = subwords\n",
    "            mask_batch[i,:len(subwords)] = 1\n",
    "            token_type_batch[i,:len(subwords)] = token_type_ids\n",
    "            subword_to_word_indices_batch[i,:len(subwords)] = subword_to_word_indices\n",
    "            seq_label_batch[i,:len(seq_label)] = seq_label\n",
    "\n",
    "            seq_list.append(raw_seq)\n",
    "            \n",
    "        return subword_batch, mask_batch, token_type_batch, subword_to_word_indices_batch, seq_label_batch, seq_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load IndoBERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.01857137680053711,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 229167,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02007a91813846eaa2b69da9db5ddab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/229k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0149993896484375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 112,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008a52971b6a4cbeb7c0f8d924d7fead",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.014726638793945312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 2,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e131a2f881c9480eb39fee76bc985dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015087604522705078,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 1534,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81a993f1ca644e988ba3a7ec54c22100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.53k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.015045166015625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading",
       "rate": null,
       "total": 497810400,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "550e3fd674614f5391216095233fa16a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/498M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForWordClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load Tokenizer and Config\n",
    "tokenizer = BertTokenizer.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config = BertConfig.from_pretrained('indobenchmark/indobert-base-p1')\n",
    "config.num_labels = QAFactoidDataset.NUM_LABELS\n",
    "\n",
    "# Instantiate model\n",
    "model = BertForWordClassification.from_pretrained('indobenchmark/indobert-base-p1', config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForWordClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(50000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "124443651"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_param(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Factoid QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset_path = '/home/facqa/dataset/facqa_qa-factoid-itb/train_preprocess.csv'\n",
    "valid_dataset_path = '/home/facqa/dataset/facqa_qa-factoid-itb/valid_preprocess.csv'\n",
    "# test_dataset_path = '/home/facqa/dataset/facqa_qa-factoid-itb/test_preprocess_masked_label.csv'\n",
    "test_dataset_path = '/home/facqa/dataset/facqa_qa-factoid-itb/test_preprocess.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = QAFactoidDataset(train_dataset_path, tokenizer, lowercase=True)\n",
    "valid_dataset = QAFactoidDataset(valid_dataset_path, tokenizer, lowercase=True)\n",
    "test_dataset = QAFactoidDataset(test_dataset_path, tokenizer, lowercase=True)\n",
    "\n",
    "train_loader = QAFactoidDataLoader(dataset=train_dataset, max_seq_len=512, batch_size=8, num_workers=7, shuffle=True)  \n",
    "valid_loader = QAFactoidDataLoader(dataset=valid_dataset, max_seq_len=512, batch_size=8, num_workers=7, shuffle=False)  \n",
    "test_loader = QAFactoidDataLoader(dataset=test_dataset, max_seq_len=512, batch_size=8, num_workers=7, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'O': 0, 'B': 1, 'I': 2}\n",
      "{0: 'O', 1: 'B', 2: 'I'}\n"
     ]
    }
   ],
   "source": [
    "w2i, i2w = QAFactoidDataset.LABEL2INDEX, QAFactoidDataset.INDEX2LABEL\n",
    "print(w2i)\n",
    "print(i2w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model on sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_subword_tokenize(question, passage, tokenizer):        \n",
    "    # Add CLS token\n",
    "    subwords = [tokenizer.cls_token_id]\n",
    "    subword_to_word_indices = [-1] # For CLS\n",
    "    token_type_ids = [0]\n",
    "\n",
    "    # Add subwords for question\n",
    "    for word_idx, word in enumerate(question):\n",
    "        subword_list = tokenizer.encode(word, add_special_tokens=False)\n",
    "        subword_to_word_indices += [-1 for i in range(len(subword_list))]\n",
    "        token_type_ids += [0 for i in range(len(subword_list))]\n",
    "        subwords += subword_list\n",
    "\n",
    "    # Add intermediate SEP token\n",
    "    subwords += [tokenizer.sep_token_id]\n",
    "    subword_to_word_indices += [-1]\n",
    "    token_type_ids += [0]\n",
    "\n",
    "    # Add subwords\n",
    "    for word_idx, word in enumerate(passage):\n",
    "        subword_list = tokenizer.encode(word, add_special_tokens=False)\n",
    "        subword_to_word_indices += [word_idx for i in range(len(subword_list))]\n",
    "        token_type_ids += [1 for i in range(len(subword_list))]\n",
    "        subwords += subword_list\n",
    "\n",
    "    # Add last SEP token\n",
    "    subwords += [tokenizer.sep_token_id]\n",
    "    subword_to_word_indices += [-1]\n",
    "    token_type_ids += [1]\n",
    "\n",
    "    return np.array(subwords), np.array(subword_to_word_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pelatih</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ganda</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>putra</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Christian</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hadinata</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tak</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>meragukan</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tekad</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Candra</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sigit</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>.</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bila</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dia</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sudah</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ngomong</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>begitu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>saya</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tidak</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ragu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lagi</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ujarnya</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>.</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words label\n",
       "0     Pelatih     I\n",
       "1       ganda     O\n",
       "2       putra     O\n",
       "3           ,     I\n",
       "4   Christian     O\n",
       "5    Hadinata     O\n",
       "6           ,     O\n",
       "7         tak     I\n",
       "8   meragukan     O\n",
       "9       tekad     O\n",
       "10     Candra     O\n",
       "11          /     O\n",
       "12      Sigit     O\n",
       "13          .     B\n",
       "14       Bila     O\n",
       "15        dia     O\n",
       "16      sudah     O\n",
       "17    ngomong     O\n",
       "18     begitu     O\n",
       "19          ,     O\n",
       "20       saya     O\n",
       "21      tidak     I\n",
       "22       ragu     O\n",
       "23       lagi     O\n",
       "24          ,     O\n",
       "25    ujarnya     O\n",
       "26          .     I"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = ['Siapakah', 'pelatih', 'ganda', 'putra', 'bulu', 'tangkis', 'yang', 'tidak', 'meragukan', 'tekad', 'Candra', '/', 'Sigit', 'untuk', 'bekerja', 'lebih', 'keras', 'pada', 'perebutan', 'piala', 'Thomas']\n",
    "passage = ['Pelatih', 'ganda', 'putra', ',', 'Christian', 'Hadinata', ',', 'tak', 'meragukan', 'tekad', 'Candra', '/', 'Sigit', '.', 'Bila', 'dia', 'sudah', 'ngomong', 'begitu', ',', 'saya', 'tidak', 'ragu', 'lagi', ',', 'ujarnya', '.']\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(question, passage, tokenizer)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "\n",
    "pd.DataFrame({'words': passage, 'label': labels})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning & Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "###\n",
    "# Training & Evaluation Function\n",
    "###\n",
    "\n",
    "# Evaluate function for validation and test\n",
    "def evaluate(model, data_loader, forward_fn, metrics_fn, i2w, is_test=False):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_labels = 0, 0, 0\n",
    "\n",
    "    list_hyp, list_label, list_seq = [], [], []\n",
    "\n",
    "    pbar = tqdm(iter(data_loader), leave=True, total=len(data_loader))\n",
    "    for i, batch_data in enumerate(pbar):\n",
    "        batch_seq = batch_data[-1]        \n",
    "        loss, batch_hyp, batch_label = forward_fn(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "        \n",
    "        # Calculate total loss\n",
    "        test_loss = loss.item()\n",
    "        total_loss = total_loss + test_loss\n",
    "\n",
    "        # Calculate evaluation metrics\n",
    "        list_hyp += batch_hyp\n",
    "        list_label += batch_label\n",
    "        list_seq += batch_seq\n",
    "        metrics = metrics_fn(list_hyp, list_label)\n",
    "\n",
    "        if not is_test:\n",
    "            pbar.set_description(\"VALID LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "        else:\n",
    "            pbar.set_description(\"TEST LOSS:{:.4f} {}\".format(total_loss/(i+1), metrics_to_string(metrics)))\n",
    "    \n",
    "    if is_test:\n",
    "        return total_loss, metrics, list_hyp, list_label, list_seq\n",
    "    else:\n",
    "        return total_loss, metrics\n",
    "\n",
    "# Training function and trainer\n",
    "def train(model, train_loader, valid_loader, optimizer, forward_fn, metrics_fn, valid_criterion, i2w, n_epochs, evaluate_every=1, early_stop=3, step_size=1, gamma=0.5, model_dir=\"\", exp_id=None):\n",
    "    # scheduler = StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "\n",
    "    best_val_metric = -100\n",
    "    count_stop = 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        total_train_loss = 0\n",
    "        list_hyp, list_label = [], []\n",
    "        \n",
    "        train_pbar = tqdm(iter(train_loader), leave=True, total=len(train_loader))\n",
    "        for i, batch_data in enumerate(train_pbar):\n",
    "            loss, batch_hyp, batch_label = forward_fn(model, batch_data[:-1], i2w=i2w, device='cuda')\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            tr_loss = loss.item()\n",
    "            total_train_loss = total_train_loss + tr_loss\n",
    "\n",
    "            # Calculate metrics\n",
    "            list_hyp += batch_hyp\n",
    "            list_label += batch_label\n",
    "            \n",
    "            train_pbar.set_description(\"(Epoch {}) TRAIN LOSS:{:.4f} LR:{:.8f}\".format((epoch+1),\n",
    "                total_train_loss/(i+1), get_lr(optimizer)))\n",
    "                        \n",
    "        metrics = metrics_fn(list_hyp, list_label)\n",
    "        print(\"(Epoch {}) TRAIN LOSS:{:.4f} {} LR:{:.8f}\".format((epoch+1),\n",
    "            total_train_loss/(i+1), metrics_to_string(metrics), get_lr(optimizer)))\n",
    "        \n",
    "        # Decay Learning Rate\n",
    "        # scheduler.step()\n",
    "\n",
    "        # evaluate\n",
    "        if ((epoch+1) % evaluate_every) == 0:\n",
    "            val_loss, val_metrics = evaluate(model, valid_loader, forward_fn, metrics_fn, i2w, is_test=False)\n",
    "\n",
    "            # Early stopping\n",
    "            val_metric = val_metrics[valid_criterion]\n",
    "            if best_val_metric < val_metric:\n",
    "                best_val_metric = val_metric\n",
    "                # save model\n",
    "                if exp_id is not None:\n",
    "                    torch.save(model.state_dict(), model_dir + \"/best_model_\" + str(exp_id) + \".th\")\n",
    "                else:\n",
    "                    torch.save(model.state_dict(), model_dir + \"/best_model.th\")\n",
    "                count_stop = 0\n",
    "            else:\n",
    "                count_stop += 1\n",
    "                print(\"count stop:\", count_stop)\n",
    "                if count_stop == early_stop:\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr= 1e-5)\n",
    "model = model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = '{}/{}/{}'.format(\"/home/facqa/save\",\"qa-factoid-itb\",\"exp\")\n",
    "if not os.path.exists(model_dir):\n",
    "    os.makedirs(model_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.2399 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 1) TRAIN LOSS:0.2399 ACC:0.95 F1:0.04 REC:0.02 PRE:0.15 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2239 ACC:0.95 F1:0.05 REC:0.03 PRE:0.18: 100%|██████████| 39/39 [00:02<00:00, 16.08it/s]\n",
      "(Epoch 2) TRAIN LOSS:0.1922 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 2) TRAIN LOSS:0.1922 ACC:0.95 F1:0.24 REC:0.20 PRE:0.30 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1986 ACC:0.94 F1:0.30 REC:0.37 PRE:0.25: 100%|██████████| 39/39 [00:02<00:00, 16.78it/s]\n",
      "(Epoch 3) TRAIN LOSS:0.1463 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 3) TRAIN LOSS:0.1463 ACC:0.97 F1:0.41 REC:0.41 PRE:0.42 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1618 ACC:0.96 F1:0.40 REC:0.43 PRE:0.37: 100%|██████████| 39/39 [00:02<00:00, 15.71it/s]\n",
      "(Epoch 4) TRAIN LOSS:0.1092 LR:0.00001000: 100%|██████████| 312/312 [00:22<00:00, 14.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 4) TRAIN LOSS:0.1092 ACC:0.98 F1:0.55 REC:0.57 PRE:0.53 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1484 ACC:0.96 F1:0.41 REC:0.42 PRE:0.41: 100%|██████████| 39/39 [00:02<00:00, 14.98it/s]\n",
      "(Epoch 5) TRAIN LOSS:0.0849 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 5) TRAIN LOSS:0.0849 ACC:0.98 F1:0.65 REC:0.68 PRE:0.62 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1509 ACC:0.96 F1:0.43 REC:0.43 PRE:0.44: 100%|██████████| 39/39 [00:02<00:00, 16.87it/s]\n",
      "(Epoch 6) TRAIN LOSS:0.0653 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6) TRAIN LOSS:0.0653 ACC:0.99 F1:0.74 REC:0.77 PRE:0.72 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1446 ACC:0.97 F1:0.51 REC:0.54 PRE:0.49: 100%|██████████| 39/39 [00:02<00:00, 16.10it/s]\n",
      "(Epoch 7) TRAIN LOSS:0.0554 LR:0.00001000: 100%|██████████| 312/312 [00:22<00:00, 14.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 7) TRAIN LOSS:0.0554 ACC:0.99 F1:0.78 REC:0.80 PRE:0.75 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1652 ACC:0.97 F1:0.52 REC:0.55 PRE:0.50: 100%|██████████| 39/39 [00:02<00:00, 15.66it/s]\n",
      "(Epoch 8) TRAIN LOSS:0.0432 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 8) TRAIN LOSS:0.0432 ACC:0.99 F1:0.82 REC:0.84 PRE:0.80 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1689 ACC:0.97 F1:0.51 REC:0.53 PRE:0.50: 100%|██████████| 39/39 [00:02<00:00, 16.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 9) TRAIN LOSS:0.0375 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 9) TRAIN LOSS:0.0375 ACC:0.99 F1:0.84 REC:0.86 PRE:0.83 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1862 ACC:0.97 F1:0.54 REC:0.59 PRE:0.50: 100%|██████████| 39/39 [00:02<00:00, 15.86it/s]\n",
      "(Epoch 10) TRAIN LOSS:0.0317 LR:0.00001000: 100%|██████████| 312/312 [00:24<00:00, 12.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 10) TRAIN LOSS:0.0317 ACC:0.99 F1:0.87 REC:0.88 PRE:0.85 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1608 ACC:0.97 F1:0.49 REC:0.50 PRE:0.48: 100%|██████████| 39/39 [00:02<00:00, 14.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 11) TRAIN LOSS:0.0297 LR:0.00001000: 100%|██████████| 312/312 [00:22<00:00, 13.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 11) TRAIN LOSS:0.0297 ACC:0.99 F1:0.87 REC:0.89 PRE:0.85 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1686 ACC:0.97 F1:0.55 REC:0.60 PRE:0.51: 100%|██████████| 39/39 [00:02<00:00, 16.51it/s]\n",
      "(Epoch 12) TRAIN LOSS:0.0271 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 12) TRAIN LOSS:0.0271 ACC:1.00 F1:0.90 REC:0.91 PRE:0.89 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1590 ACC:0.97 F1:0.52 REC:0.54 PRE:0.50: 100%|██████████| 39/39 [00:02<00:00, 16.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 13) TRAIN LOSS:0.0228 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 13) TRAIN LOSS:0.0228 ACC:1.00 F1:0.90 REC:0.91 PRE:0.89 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1842 ACC:0.97 F1:0.55 REC:0.60 PRE:0.51: 100%|██████████| 39/39 [00:02<00:00, 16.76it/s]\n",
      "(Epoch 14) TRAIN LOSS:0.0208 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 14) TRAIN LOSS:0.0208 ACC:1.00 F1:0.91 REC:0.92 PRE:0.90 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1495 ACC:0.97 F1:0.52 REC:0.58 PRE:0.46: 100%|██████████| 39/39 [00:02<00:00, 15.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 15) TRAIN LOSS:0.0184 LR:0.00001000: 100%|██████████| 312/312 [00:21<00:00, 14.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 15) TRAIN LOSS:0.0184 ACC:1.00 F1:0.92 REC:0.93 PRE:0.90 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1860 ACC:0.97 F1:0.54 REC:0.60 PRE:0.49: 100%|██████████| 39/39 [00:02<00:00, 15.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 16) TRAIN LOSS:0.0166 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 16) TRAIN LOSS:0.0166 ACC:1.00 F1:0.94 REC:0.95 PRE:0.93 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1951 ACC:0.96 F1:0.53 REC:0.58 PRE:0.49: 100%|██████████| 39/39 [00:02<00:00, 15.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.0165 LR:0.00001000: 100%|██████████| 312/312 [00:22<00:00, 13.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 17) TRAIN LOSS:0.0165 ACC:1.00 F1:0.93 REC:0.94 PRE:0.91 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1786 ACC:0.97 F1:0.55 REC:0.59 PRE:0.52: 100%|██████████| 39/39 [00:02<00:00, 15.32it/s]\n",
      "(Epoch 18) TRAIN LOSS:0.0147 LR:0.00001000: 100%|██████████| 312/312 [00:22<00:00, 13.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 18) TRAIN LOSS:0.0147 ACC:1.00 F1:0.93 REC:0.94 PRE:0.92 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1645 ACC:0.96 F1:0.51 REC:0.55 PRE:0.48: 100%|██████████| 39/39 [00:02<00:00, 15.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 19) TRAIN LOSS:0.0132 LR:0.00001000: 100%|██████████| 312/312 [00:25<00:00, 12.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 19) TRAIN LOSS:0.0132 ACC:1.00 F1:0.94 REC:0.95 PRE:0.93 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1720 ACC:0.97 F1:0.53 REC:0.58 PRE:0.49: 100%|██████████| 39/39 [00:02<00:00, 15.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 20) TRAIN LOSS:0.0115 LR:0.00001000: 100%|██████████| 312/312 [00:22<00:00, 13.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 20) TRAIN LOSS:0.0115 ACC:1.00 F1:0.95 REC:0.96 PRE:0.94 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1669 ACC:0.97 F1:0.54 REC:0.58 PRE:0.50: 100%|██████████| 39/39 [00:02<00:00, 17.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 21) TRAIN LOSS:0.0104 LR:0.00001000: 100%|██████████| 312/312 [00:22<00:00, 13.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21) TRAIN LOSS:0.0104 ACC:1.00 F1:0.95 REC:0.96 PRE:0.94 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1885 ACC:0.96 F1:0.51 REC:0.57 PRE:0.47: 100%|██████████| 39/39 [00:02<00:00, 16.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 22) TRAIN LOSS:0.0114 LR:0.00001000: 100%|██████████| 312/312 [00:22<00:00, 13.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 22) TRAIN LOSS:0.0114 ACC:1.00 F1:0.95 REC:0.96 PRE:0.94 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1939 ACC:0.97 F1:0.57 REC:0.62 PRE:0.54: 100%|██████████| 39/39 [00:02<00:00, 16.24it/s]\n",
      "(Epoch 23) TRAIN LOSS:0.0094 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 23) TRAIN LOSS:0.0094 ACC:1.00 F1:0.96 REC:0.96 PRE:0.95 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.1880 ACC:0.97 F1:0.55 REC:0.56 PRE:0.53: 100%|██████████| 39/39 [00:02<00:00, 15.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 24) TRAIN LOSS:0.0090 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 24) TRAIN LOSS:0.0090 ACC:1.00 F1:0.96 REC:0.97 PRE:0.95 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2154 ACC:0.97 F1:0.56 REC:0.59 PRE:0.53: 100%|██████████| 39/39 [00:02<00:00, 16.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "(Epoch 25) TRAIN LOSS:0.0090 LR:0.00001000: 100%|██████████| 312/312 [00:23<00:00, 13.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 25) TRAIN LOSS:0.0090 ACC:1.00 F1:0.95 REC:0.96 PRE:0.94 LR:0.00001000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "VALID LOSS:0.2090 ACC:0.97 F1:0.56 REC:0.60 PRE:0.52: 100%|██████████| 39/39 [00:02<00:00, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count stop: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "set_seed(42)\n",
    "train(\n",
    "    model,\n",
    "    train_loader=train_loader,\n",
    "    valid_loader=valid_loader,\n",
    "    optimizer=optimizer,\n",
    "    forward_fn=forward_word_classification,\n",
    "    metrics_fn=qa_factoid_metrics_fn,\n",
    "    valid_criterion='F1',\n",
    "    i2w=i2w,\n",
    "    n_epochs=25,\n",
    "    evaluate_every=1,\n",
    "    early_stop=12,\n",
    "    step_size=1,\n",
    "    gamma=0.5,\n",
    "    model_dir=model_dir,\n",
    "    exp_id=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=========== EVALUATION PHASE ===========\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TEST LOSS:0.1801 ACC:0.97 F1:0.55 REC:0.58 PRE:0.52: 100%|██████████| 39/39 [00:02<00:00, 16.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Prediction Result ==\n",
      "                                                 seq  \\\n",
      "0  Siapakah pelatih ganda putra bulu tangkis yang...   \n",
      "1  Siapa nama Perdana Menteri Inggris ?|Curtis me...   \n",
      "2  Berapakah catatan waktu terbaik Tonique Willia...   \n",
      "3  Apakah nama latin dari kijang|Dalam waktu sebu...   \n",
      "4  Siapa nama presiden Indonesia sekarang ?|Presi...   \n",
      "\n",
      "                                                 hyp  \\\n",
      "0  [O, O, O, O, B, I, O, O, O, O, O, O, O, O, O, ...   \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
      "4  [O, B, I, I, O, O, O, B, I, O, O, O, O, O, O, ...   \n",
      "\n",
      "                                               label  \n",
      "0  [O, O, O, O, B, I, O, O, O, O, O, O, O, O, O, ...  \n",
      "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "3  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "4  [O, B, I, I, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
      "\n",
      "== Model Performance ==\n",
      "            ACC        F1       REC      PRE\n",
      "count  1.000000  1.000000  1.000000  1.00000\n",
      "mean   0.965898  0.545723  0.579937  0.51532\n",
      "std         NaN       NaN       NaN      NaN\n",
      "min    0.965898  0.545723  0.579937  0.51532\n",
      "25%    0.965898  0.545723  0.579937  0.51532\n",
      "50%    0.965898  0.545723  0.579937  0.51532\n",
      "75%    0.965898  0.545723  0.579937  0.51532\n",
      "max    0.965898  0.545723  0.579937  0.51532\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load(model_dir + \"/best_model_0.th\"))\n",
    "\n",
    "# Evaluate\n",
    "print(\"=========== EVALUATION PHASE ===========\")\n",
    "test_loss, test_metrics, test_hyp, test_label, test_seq = evaluate(\n",
    "    model, data_loader=test_loader, forward_fn=forward_word_classification, metrics_fn=qa_factoid_metrics_fn, i2w=i2w, is_test=True\n",
    ")\n",
    "\n",
    "# dftrue = pd.read_csv(\"./dataset/facqa_qa-factoid-itb/test_preprocess.csv\")\n",
    "# test_label = dftrue[\"seq_label\"]\n",
    "# test_metrics = qa_factoid_metrics_fn(test_hyp, test_label)\n",
    "\n",
    "metrics_scores = []\n",
    "result_dfs = []\n",
    "\n",
    "metrics_scores.append(test_metrics)\n",
    "result_dfs.append(pd.DataFrame({\n",
    "    'seq':test_seq, \n",
    "    'hyp': test_hyp, \n",
    "    'label': test_label\n",
    "}))\n",
    "\n",
    "result_df = pd.concat(result_dfs)\n",
    "metric_df = pd.DataFrame.from_records(metrics_scores)\n",
    "\n",
    "print('== Prediction Result ==')\n",
    "print(result_df.head())\n",
    "print()\n",
    "\n",
    "print('== Model Performance ==')\n",
    "print(metric_df.describe())\n",
    "\n",
    "result_df.to_csv(model_dir + \"/prediction_result.csv\")\n",
    "metric_df.describe().to_csv(model_dir + \"/evaluation_result.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test fine-tuned model with sample sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question:  Siapakah pelatih ganda putra bulu tangkis yang tidak meragukan tekad Candra / Sigit untuk bekerja lebih keras pada perebutan piala Thomas\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pelatih</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ganda</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>putra</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Christian</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hadinata</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>tak</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>meragukan</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tekad</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Candra</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>/</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Sigit</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Bila</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>dia</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>sudah</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>ngomong</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>begitu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>saya</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>tidak</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ragu</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>lagi</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>,</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>ujarnya</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>.</td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        words label\n",
       "0     Pelatih     O\n",
       "1       ganda     O\n",
       "2       putra     O\n",
       "3           ,     O\n",
       "4   Christian     B\n",
       "5    Hadinata     I\n",
       "6           ,     O\n",
       "7         tak     O\n",
       "8   meragukan     O\n",
       "9       tekad     O\n",
       "10     Candra     O\n",
       "11          /     O\n",
       "12      Sigit     O\n",
       "13          .     O\n",
       "14       Bila     O\n",
       "15        dia     O\n",
       "16      sudah     O\n",
       "17    ngomong     O\n",
       "18     begitu     O\n",
       "19          ,     O\n",
       "20       saya     O\n",
       "21      tidak     O\n",
       "22       ragu     O\n",
       "23       lagi     O\n",
       "24          ,     O\n",
       "25    ujarnya     O\n",
       "26          .     O"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = ['Siapakah', 'pelatih', 'ganda', 'putra', 'bulu', 'tangkis', 'yang', 'tidak', 'meragukan', 'tekad', 'Candra', '/', 'Sigit', 'untuk', 'bekerja', 'lebih', 'keras', 'pada', 'perebutan', 'piala', 'Thomas']\n",
    "passage = ['Pelatih', 'ganda', 'putra', ',', 'Christian', 'Hadinata', ',', 'tak', 'meragukan', 'tekad', 'Candra', '/', 'Sigit', '.', 'Bila', 'dia', 'sudah', 'ngomong', 'begitu', ',', 'saya', 'tidak', 'ragu', 'lagi', ',', 'ujarnya', '.']\n",
    "subwords, subword_to_word_indices = word_subword_tokenize(question, passage, tokenizer)\n",
    "subwords = torch.LongTensor(subwords).view(1, -1).to(model.device)\n",
    "subword_to_word_indices = torch.LongTensor(subword_to_word_indices).view(1, -1).to(model.device)\n",
    "logits = model(subwords, subword_to_word_indices)[0]\n",
    "\n",
    "preds = torch.topk(logits, k=1, dim=-1)[1].squeeze().cpu().numpy()\n",
    "labels = [i2w[preds[i]] for i in range(len(preds))]\n",
    "print(\"Question: \", \" \".join(question))\n",
    "pd.DataFrame({'words': passage, 'label': labels})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
